
> ###########################################
> # Econometrics - Problem Set 3            #
> # Students: Luan Borelli and Diogo Wolff  #
> ###########################################
> 
> ######
> # Q6 #
> ######
> 
> # Defining Nelder-mead parameters:
> reflection <- 1

> expansion <- 1.7

> contraction <- 0.3 

> shrinkage <- 0.6

> # Defining tolerance variables:
> max_run <- 100 # Loop max run times.

> run_count <- 0 # Loop counter.

> tolerance <- 10^(-6) # Tolerance.

> tol <- 999999999999 # An initial tolerance value to be updated.

> # Defining the Himmelblau function, to be minimized:
> f <- function(theta) { 
+   (theta[1]^2 + theta[2] - 11)^2 + (theta[1]+theta[2]^2 - 7)^2
+ }

> # Defining the algorithm function.
> # This function receives a list of three 2-dimensional vectors (i.e., a list of vertices)
> # E.g.: list(c(2, 3), c(7, 3), c(6,4))
> 
> nelder_mead <- function(verts) {
+   vertices <- verts
+   while(run_count <= max_run & tol >= tolerance) {
+     
+     f_at_vertices <- sapply(vertices,f)
+     best_pos <- which.min(f_at_vertices)
+     worst_pos <- which.max(f_at_vertices)
+     new_vertices <- vertices[-worst_pos]
+     f_at_new_vertices <- sapply(new_vertices, f)
+     
+     centroid <- 1/length(new_vertices) * (unlist(new_vertices[1]) + unlist(new_vertices[2]))
+     reflection_point <- (1+reflection)*centroid - reflection*unlist(vertices[worst_pos])
+     
+     if(f(reflection_point) < f_at_vertices[best_pos]) {
+       theta_e = (1 + reflection*expansion)*centroid - reflection*expansion*unlist(vertices[worst_pos])
+       if(f(theta_e) < f(reflection_point)) {
+         vertices <- c(new_vertices, list(theta_e))
+       } else {
+         vertices <- c(new_vertices, list(reflection_point))
+       }
+     } else if(f(reflection_point) >= f_at_vertices[best_pos] & f(reflection_point) < f(unlist(new_vertices[which.max(f_at_new_vertices)]))) {
+       vertices <- c(new_vertices, list(reflection_point))  
+     } else if(f(unlist(new_vertices[which.max(f_at_new_vertices)])) <= f(reflection_point & f(reflection_point) < f_at_vertices[worst_pos])) {
+       theta_c <- (1 + reflection*contraction)*centroid - reflection*contraction*unlist(vertices[worst_pos])
+       if(f(theta_c) < f(reflection_point)) {
+         vertices <- c(new_vertices, list(theta_c))  
+       } else {
+         tilde_theta_1 <- unlist(vertices[best_pos])
+         tilde_theta_2 <- shrinkage*unlist(vertices[best_pos]) + (1-shrinkage)*unlist(new_vertices[which.max(f_at_new_vertices)])
+         tilde_theta_3 <- shrinkage*unlist(vertices[best_pos]) + (1-shrinkage)*unlist(vertices[worst_pos])
+         vertices <- list(tilde_theta_1, tilde_theta_2, tilde_theta_3)
+       } 
+     } else if(f(reflection_point) >= f_at_vertices[worst_pos]) {
+       theta_c2 <- (1 - contraction)*centroid + contraction*unlist(vertices[worst_pos])
+       if(f(theta_c2)<f_at_vertices[worst_pos]) {
+         vertices <- c(new_vertices, list(theta_c2))
+       } else {
+         tilde_theta_1 <- unlist(vertices[best_pos])
+         tilde_theta_2 <- shrinkage*unlist(vertices[best_pos]) + (1-shrinkage)*unlist(new_vertices[which.max(f_at_new_vertices)])
+         tilde_theta_3 <- shrinkage*unlist(vertices[best_pos]) + (1-shrinkage)*unlist(vertices[worst_pos])
+         vertices <- list(tilde_theta_1, tilde_theta_2, tilde_theta_3)
+       }
+     }
+     run_count = run_count + 1
+     tol = max(sapply(vertices,f)) - min(sapply(vertices,f))
+     # print(c("Round", run_count))
+   }
+   
+   print('Minimizers (final algorithm vertices):')
+   print(vertices)
+   print('Minimized function (value function) at each vertice:')
+   print(format(sapply(vertices,f), scientific=F))
+ }

> ### RESULTS
> 
> # This function has four identical local minima and one local maximum. 
> # Here I use as initial guesses three different sets of vertices. 
> # Each guess results in a different minimum point.
> 
> set_1 <- list(c(-0.2, -1), c(-0.3, -0.9), c(0,-0.5))

> set_2 <- list(c(2, 3), c(7, 3), c(6,4))

> set_3 <- list(c(1, 7), c(2, 1), c(3,5))

> # Calling the function for minimization at each of the three above sets: 
> 
> nelder_mead(set_1) # Result: (x, y) = (3,2), f(x, y) = 0
[1] "Minimizers (final algorithm vertices):"
[[1]]
[1] 2.999983 2.000077

[[2]]
[1] 3.000166 1.999943

[[3]]
[1] 3.000042 2.000092

[1] "Minimized function (value function) at each vertice:"
[1] "0.00000008475889" "0.00000089127390" "0.00000028809130"

> nelder_mead(set_2) # Result: (x, y) = (-2.8, 3.13), f(x, y) = 0
[1] "Minimizers (final algorithm vertices):"
[[1]]
[1] -2.804994  3.131360

[[2]]
[1] -2.805114  3.131375

[[3]]
[1] -2.805089  3.131444

[1] "Minimized function (value function) at each vertice:"
[1] "0.0000005972274" "0.0000001586661" "0.0000007267577"

> nelder_mead(set_3) # Result: (x, y) = (3.58, -1.84), f(x, y) = 0
[1] "Minimizers (final algorithm vertices):"
[[1]]
[1]  3.584418 -1.848000

[[2]]
[1]  3.584510 -1.847934

[[3]]
[1]  3.584503 -1.848167

[1] "Minimized function (value function) at each vertice:"
[1] "0.0000002319486" "0.0000010018925" "0.0000002987836"

> # The other minimum point could be attained by:
> # nelder_mead(list(c(-5, -4), c(-3, -2), c(-1,-4))) # Result: (x, y) = (-3.77. -3.2), f(x, y) = 0
> 
> 
> ######
> # Q7 #
> ######
> 
> 
> # Loading data: 
> 
> prisoner = readRDS('PS3/prisoner.Rds')

> # Saving eulers constant:
> euler = -digamma(1)

> # The functions below calculate the CCPs for a given set of parameters and prison times.
> # The first one calculates the formulas presented on the PDF, and the second is a wrapper
> # that applies the first to the dataset, and also calculates the conditional probability
> # for t = 2.
> 
> calculates_ccps_given_t = function(alpha, alpha1, alpha2, t1, t2) {
+   U_deny_t1 = alpha*(alpha1*10 + alpha2*100)
+   U_deny_t2 = alpha*(alpha1*10 + alpha2*100)
+   U_confess_t1 = alpha1*t1 + alpha2*(t1)^2
+   U_confess_t2 = alpha1*t2 + alpha2*(t2)^2
+   v_deny = max(U_deny_t1 + euler, U_confess_t2 + euler)
+   V_confess = euler
+   
+   ccp_confess_t1 = exp(V_confess + U_confess_t1)/(exp(V_confess + U_confess_t1) + 
+                                                     exp(v_deny + U_deny_t1))
+   
+   ccp_confess_t2 = exp(U_confess_t2)/(exp(U_confess_t2) + exp(U_deny_t2))
+   
+   return(c(ccp_confess_t1, ccp_confess_t2))
+ }

> calculates_ccps_of_data = function(alpha, alpha1, alpha2, data) {
+   ccps = calculates_ccps_given_t(alpha, alpha1, alpha2, data$offer1, data$offer2)
+   
+   ccps[2] = ifelse(data$choice1 == "confess", 1, ccps[2])
+   
+   return(ccps)
+ }

> # Given the CCP functions, we calculate the log-likelihood of a given param vector
> # by applying the functions to every data point, and then use the standard log-lik formula.
> 
> log_lik_function = function(params, data) {
+   confess_probabilities = do.call('rbind', purrr::map(1:1000, 
+                                                ~ calculates_ccps_of_data(params[1], params[2], params[3], 
+                                                                          data[.x,])))
+   
+   df_with_probabilities = cbind(data, confess_probabilities)
+   
+   -sum(log(ifelse(df_with_probabilities$choice1 == "deny", 
+                   1-df_with_probabilities$`1`, 
+                   df_with_probabilities$`1`)*
+              ifelse(df_with_probabilities$choice2 == "deny", 
+                     1-df_with_probabilities$`2`, 
+                     df_with_probabilities$`2`)))
+   
+ }

> # To find the estimate, we simply use a standard optimizer:
> result = optim(c(1/2, -1, -0.2), log_lik_function, data = prisoner)

> print(result)
$par
[1]  0.15655623 -3.56879369 -0.08882856

$value
[1] 204.9842

$counts
function gradient 
     174       NA 

$convergence
[1] 0

$message
NULL

> # Estimated results:
> # alpha = 0.15655623
> # alpha_1 = -3.56879369
> # alpha_2 = -0.08882856

> ######
> # Q8 #
> ######
> 
> 
> # To generate bootstrap estimations, we follow the steps outlined in class,
> # creating a for loop that resamples the data, runs the estimator, and stores the
> # parameters found.
> 
> B = 100

> for (b in 1:B) {
+   
+   if (b%%10==0){print(b)}
+   set.seed(500+b)
+   
+   bootsample = sample(1:1000, 1000, replace=TRUE)
+   boot.data = prisoner[bootsample,]
+   
+   bootstrap_coefficients = optim(c(1/2, -1, -0.2), log_lik_function, data = boot.data)
+   if (b==1) {boot.coef = bootstrap_coefficients$par} else
+     {boot.coef = c(boot.coef, bootstrap_coefficients$par)}
+ }
[1] 10
[1] 20
[1] 30
[1] 40
[1] 50
[1] 60
[1] 70
[1] 80
[1] 90
[1] 100

> mat.boot = matrix(boot.coef, ncol = 3, byrow = TRUE)

> # Then, we generate the varcov matrix:
> 
> varcov = t(apply(mat.boot, 2, scale, scale=FALSE, center=TRUE))%*%
+   apply(mat.boot, 2, scale, scale=FALSE, center=TRUE)/99

> # The estimates and variances are checked below,
> 
> mean_est = colMeans(mat.boot)

> sd_est = sqrt(diag(varcov))

> print(mean_est)
[1]  0.15885430 -3.66473810 -0.09048549

> print(sd_est)
[1] 0.01756616 0.34668090 0.06635169

> # as are each requested confidence intervals:
> 
> alpha_conf_int = c(sort(mat.boot[,1])[6], sort(mat.boot[,1])[95])

> alpha1_conf_int = c(sort(mat.boot[,2])[6], sort(mat.boot[,2])[95])

> alpha2_conf_int = c(sort(mat.boot[,3])[6], sort(mat.boot[,3])[95])

> print(alpha_conf_int)
[1] 0.1319387 0.1914511

> print(alpha1_conf_int)
[1] -4.231291 -3.155847

> print(alpha2_conf_int)
[1] -0.19095885  0.01703139
